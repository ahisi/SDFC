\documentclass[12pt,a4paper]{article}

%% Packages {{{

\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[natbib=true,
            doi=false,
            isbn=false,
            url=false,
            doi=false,
            eprint=false,
            style=authoryear,
            maxnames=2,
            firstinits=true,
            backend=bibtex]{biblatex}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{longtable}
\usepackage{rotating}
\usepackage[]{algorithm2e}
\usepackage{listings}

%%}}}

%% Source code macros %% {{{

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{pcr}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{pcr}{m}{n}{12}  % for normal

% Custom colors
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}} }

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}


%%}}}

%% Theorem macros %% {{{

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}

%%}}}

%% Preamble {{{

\renewcommand{\arraystretch}{1.2}

\bibliography{bibliography}
\onehalfspacing

\title{Description of the methods used in the SDFC package}
\author{Robin Y.}
\date{\today}

%%}}}

%% Macros %% {{{

\SetKw{KwFrom}{from}
\SetKw{KwInit}{Initialization:}
\SetKw{KwStep}{Step:}

\newcommand\TODO[1]{\textcolor{red}{TODO #1}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\lL}{\mathcal{L}}
\newcommand{\nN}{\mathcal{N}}
\newcommand{\zZ}{\mathcal{Z}}

\newcommand{\NL}{\mathbf{NL}}
\newcommand{\GEV}{\mathrm{GEV}}


%% }}}

\begin{document}

%% Title %% {{{

\maketitle

%%}}}

%% Abstract %% {{{

\begin{abstract}
\end{abstract}

%% }}}

%% TODO % {{{


%%}}}

%% Table of contents %% {{{

\tableofcontents

%%}}}

\section*{Introduction} %%{{{
\addcontentsline{toc}{section}{Introduction}

\noindent Our goal is to find the parameters of a probability distribution, e.g.
for a Normal distribution $\nN(\mu,\sigma)$ to find $\mu$ and $\sigma$ (see.
Sec.~\ref{sec::normal} for the definition). We place ourselves in a very general
context: we assumes that $\mu$ and $\sigma$ can vary with time $t$, and depend
of ``true'' parameters $\theta$. Mathematically, we write:

\[
\begin{pmatrix} \mu_t \\ \sigma_t \end{pmatrix}
= \lL(\theta,X_t)
= \begin{pmatrix} \lL_\mu(\theta,X_t) \\ \lL_\sigma(\theta,X_t)\end{pmatrix}
\]

\noindent The variable $\theta\in\RR^p$ is the parameter that we want to infer,
$X_t$ is called a \emph{co-variable}, which describes the time dependance. The
function $\lL$ is called the \emph{link function}.


%%}}}

\section*{Notations} %%{{{
\addcontentsline{toc}{section}{Notations}

\[Z = \frac{Y-\mu}{\sigma}\]
\[\zZ = 1+\xi Z\]

%%}}}

\section{Generic estimation} %%{{{

\subsection{Maximum likelihood estimation} %%{{{

%%}}}

\subsection{Bayesian estimation} %%{{{

%%}}}

%%}}}

\section{Normal distribution}\label{sec::normal} %%{{{

\subsection{Definition} %%{{{

\begin{definition}[Normal distribution] A random variable $Y$ follows
the \emph{Normal distribution}, noted $Y\sim\nN(\mu,\sigma)$,
$\mu\in\RR$, $\sigma>0$ if

\[
\begin{aligned}
\PP(Y\leq y) &:= \int_{-\infty}^y f(\tilde{y}).\mathrm{d}\tilde{y}\\
f(y) &:= \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{1}{2}Z^2 \right]
\end{aligned}
\]

\noindent The function $f$ is the density of the Normal distribution, $\mu$ is
called the \emph{location} parameter, and $\sigma$ is called the \emph{scale}
parameter.

\end{definition}

%%}}}

\subsection{Likelihood function} %%{{{

\noindent For a time series of observations $(Y_k)_{k=1\dots K}$, the negative
log-likelihood function is given by:

\[\NL(Y_k):=\sum_{k=1}^K
\frac{1}{2}Z_k^2 + \log(\sigma_k)\]

\begin{proposition} The gradient of the negative log-likelihood of the Normal
distribution is given by

\[
\partial_{\theta_j}\NL(Y_k)=\sum_{k=1}^K
\partial_{\theta_j}\mu\left[-\frac{Z_k}{\sigma_k}\right] +
\partial_{\theta_j}\sigma\left[-\frac{Y_kZ_k}{\sigma_k^2}+\frac{\mu_kZ_k}{\sigma_k^2}+\frac{1}{\sigma_k}\right]
\]

\end{proposition}

\begin{proof} Start with the gradient in the direction $\theta_j$

\[
\partial_{\theta_j}\NL(Y_k)=\sum_{k=1}^K \partial_{\theta_j}\left[ Z_k\right] Z_k +
\frac{\partial_{\theta_j}\sigma_k}{\sigma_k}
\]

\noindent We have

\[
\partial_{\theta_j}Z_k = - \frac{Y_k}{\sigma_k^2}\partial_{\theta_j}\sigma_k -
\frac{\sigma_k\partial_{\theta_j}\mu_k-\mu_k\partial_{\theta_j}\sigma_k}{\sigma_k^2}
\]

\noindent Consequently

\[
\begin{aligned}
\partial_{\theta_j}\NL(Y_k) &= \sum_{k=1}^K
\left[ - \frac{Y_k}{\sigma_k^2}\partial_{\theta_j}\sigma_k - \frac{\partial_{\theta_j}\mu_k}{\sigma_k} + \frac{\mu_k}{\sigma_k^2}\partial_{\theta_j}\sigma_k\right] Z_k +
\frac{\partial_{\theta_j}\sigma_k}{\sigma_k}\\
&= \partial_{\theta_j}\mu\left[-\frac{Z_k}{\sigma_k}\right] +
\partial_{\theta_j}\sigma\left[-\frac{Y_kZ_k}{\sigma_k^2}+\frac{\mu_kZ_k}{\sigma_k^2}+\frac{1}{\sigma_k}\right]
\end{aligned}
\]

\end{proof}

%%}}}

%%}}}

\section{GEV distribution} %% {{{


\subsection{Definition} %%{{{

\begin{definition}[Normal distribution] A random variable $Y$ follows the
\emph{Generalized Extreme Value} distribution (GEV), noted
$Y\sim\GEV(\mu,\sigma,\xi)$, $\mu,\xi\in\RR$, $\sigma>0$ if

\[
\begin{aligned}
\PP(Y\leq y) &:= \exp\left[-\zZ^{-1/\xi}\right] =\int_{-\infty}^y f(\tilde{y}).\mathrm{d}\tilde{y}\\
f(y) &:= \frac{1}{\sigma}\zZ^{-1-1/\xi}\exp\left[\zZ^{-1/\xi}\right]
\end{aligned}
\]

\noindent The function $f$ is the density of the GEV distribution, $\mu$ is
called the \emph{location} parameter, $\sigma$ is called the \emph{scale}
parameter and $\xi$ is called the \emph{shape} parameter.

\end{definition}

%%}}}

\subsection{Likelihood function} %%{{{

\noindent For a time series of observations $(Y_k)_{k=1\dots K}$, the negative
log-likelihood function is given by:

\[\mathbf{NL}(Y_k):=\sum_{k=1}^K
\left(1+\frac{1}{\xi_k}\right)\log(\zZ_k) + 
\zZ_k^{-1/\xi_k} +
\log(\sigma_k)
\]

\begin{proposition} The gradient of the negative log-likelihood of the GEV
distribution is given by

\[
\begin{aligned}
\partial_{\theta_j}\mathbf{NL}(Y_k) &= \sum_{k=1}^K
\partial_{\theta_j}\mu
\left[-\frac{\xi_k}{\sigma_k}\kappa_k\right] +
\partial_{\theta_j}\sigma
\left[\frac{1}{\sigma_k}-\xi_k\frac{Z_k}{\sigma_k}\kappa_k\right] +
\partial_{\theta_j}\xi
\left[\log(\zZ_k)\frac{\zZ_k^{-1/\xi_k}-1}{\xi_k^2}+Z_k\kappa_k\right]\\
\kappa_k &=
\frac{1}{\zZ_k}\left(1+\frac{1}{\xi_k}\right)-\frac{\zZ_k^{-1/\xi_k-1}}{\xi_k}
\end{aligned}
\]
\end{proposition}

\begin{proof} Start with the gradient in the direction $\theta_j$ %%{{{

\[
\begin{aligned}
\partial_{\theta_j}\NL(Y_k) & =\sum_{k=1}^K
\partial_{\theta_j}\left[\left(1+\frac{1}{\xi_k}\right)\log(\zZ_k)\right] + 
\partial_{\theta_j}\left[\zZ_k^{-1/\xi_k}\right] +
\partial_{\theta_j}\log(\sigma_k)\\
& =\sum_{k=1}^K
\left[-\frac{\partial_{\theta_j}\xi_k}{\xi_k^2}\log(\zZ_k) + 
\left(1+\frac{1}{\xi_k}\right)\frac{\partial_{\theta_j}\zZ_k}{\zZ_k}\right] +
\partial_{\theta_j}\left[\zZ_k^{-1/\xi_k}\right] + 
\frac{\partial_{\theta_j}\sigma_k}{\sigma_k}
\end{aligned}
\]

\noindent The term $\partial_{\theta_k}\zZ_k$ is given by

\[
\begin{aligned}
\partial_{\theta_j}\zZ_k & =\partial_{\theta_j}\left[1+\xi_k\frac{Y_k-\mu_k}{\sigma_k}\right]\\
 & =Z_k\partial_{\theta_j}\xi_k+\xi_k\partial_{\theta_j}\frac{Y_k-\mu_k}{\sigma_k}\\
 &
 =Z_k\partial_{\theta_j}\xi_k+\xi_k\left[-Y_k\frac{\partial_{\theta_j}\sigma_k}{\sigma_k^2}-\frac{\sigma_k\partial_{\theta_j}\mu_k-\mu_k\partial_{\theta_j}\sigma_k}{\sigma_k^2}\right]\\
 &
 =\partial_{\theta_j}\mu_k\left[-\frac{\xi_k}{\sigma_k}\right]+\partial_{\theta_j}\sigma_k\left[\xi_k\frac{\mu_k-Y_k}{\sigma_k^2}\right]+\partial_{\theta_j}\xi_k\left[Z_k\right]
\end{aligned}
\]

\noindent And the term $\partial_{\theta_k}\zZ_k^{-1/\xi_k}$ is given by

\[
\begin{aligned}
\partial_{\theta_j}\left[\zZ_k^{-1/\xi_k}\right]
&= \partial_{\theta_j}\exp\left[-\frac{1}{\xi_k}\log\left[\zZ_k\right]\right]\\
&= \partial_{\theta_j}\left[-\frac{1}{\xi_k}\log\left[\zZ_k\right]\right]\zZ_k^{-1/\xi_k}\\
&= \left[\frac{\partial_{\theta_j}\xi_k}{\xi_k^2}\log\left[\zZ_k\right] -
\frac{1}{\xi_k}\frac{\partial_{\theta_j}\zZ_k}{\zZ_k}\right]\zZ_k^{-1/\xi_k}\\
&=
\partial_{\theta_j}\xi_k\left[\log\left[\zZ_k\right]\frac{\zZ_{k}^{-1/\xi_k}}{\xi_k^2}\right] -
\frac{\zZ_k^{-1/\xi_k-1}}{\xi_k}\partial_{\theta_j}\zZ_k
\end{aligned}
\]

\noindent Thus, we find

\[
\begin{aligned}
\partial_{\theta_j}\NL(Y_k) & =\sum_{k=1}^K -
\frac{\partial_{\theta_j}\xi_k}{\xi_k^2}\log(\zZ_k) +
\left(1+\frac{1}{\xi_k}\right)\frac{\partial_{\theta_j}\zZ_k}{\zZ_k} + 
\partial_{\theta_j}\left[\zZ_k^{-1/\xi_k}\right] +
\frac{\partial_{\theta_j}\sigma_k}{\sigma_k} \\
&= \sum_{k=1}^K
\partial_{\theta_j}\xi_k\left[\log\left[\zZ_k\right]\frac{\zZ_{k}^{-1/\xi_k}-1}{\xi_k^2}\right] + 
\underbrace{\left[\frac{1}{\zZ_k}\left(1+\frac{1}{\xi_k}\right)-\frac{\zZ_k^{-1/\xi_k-1}}{\xi_k}\right]}_{=\kappa_k}
\partial_{\theta_j}\zZ_k+\frac{\partial_{\theta_j}\sigma_k}{\sigma_k}\\
&= \sum_{k=1}^K
\partial_{\theta_j}\mu_k\left[-\frac{\xi_k}{\sigma_k}\kappa_k\right] +
\partial_{\theta_j}\sigma_k\left[\frac{1}{\sigma_k}-{\xi_k}\frac{Z_k}{\sigma_k}\kappa_k\right] +
\partial_{\theta_j}\xi_k\left[\log\left[\zZ_k\right]\frac{\zZ_k^{-1/\xi_k}-1}{\xi_k^2}+Z_k\kappa_k\right]
\end{aligned}
\]

\end{proof}
%%}}}

%%}}}

\subsection{$L$-moments estimator} %%{{{

\noindent See~\citet{Hosking1985}.

%%}}}

%%}}}

\section*{Summary} %% {{{
\addcontentsline{toc}{section}{Summary}

%%}}}

%% Appendix %% {{{

\clearpage
\newpage
\pagebreak[4]

\appendix

%%}}}

%% Bibliography %% {{{
\clearpage
\newpage
\pagebreak[4]
\printbibliography

%%}}}

%% Break {{{

\clearpage
\newpage
\pagebreak[4]

%% }}}

\section*{Tabulars} %%{{{

\clearpage
\newpage
\pagebreak[4]
%%}}}

\section*{Figures} %%{{{


%\begin{figure}[!h]
%\centering
%\includegraphics[width=0.8\linewidth]{figures/fig01}
%\caption{
%}
%\label{fig::fig01}
%\end{figure}

%%}}}



\end{document}
